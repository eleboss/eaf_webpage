<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Autofocus for Event Cameras</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:50px">Autofocus for Event Cameras</span>
		<table align=center width=600px>
			<table align=center width=900px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:25px"><a href="https://github.com/eleboss">Shijie Lin</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:25px"><a href="https://yinqiangzhang.netlify.app/">Yinqiang Zhang</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:25px"><a href="http://dvs-whu.cn/">Lei Yu</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:25px"><a href="https://scholar.google.com/citations?user=tG4RnyYAAAAJ&hl=en">Bin Zhou</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:25px"><a href="http://xiaoweiluo.com/">Xiaowei Luo</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:25px"><a href="https://sites.google.com/site/panjia/home">Jia Pan</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/pdf/2203.12321'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='appendix.pdf'>[Appendix]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/eleboss/eaf_code'>[GitHub]</a></span><br>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://doi.org/10.25442/hku.19407884.v1'>[Dataset]</a></span><br>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://youtu.be/_BL80phQMW8'>[Youtube]</a></span><br>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://www.bilibili.com/video/BV1g34y147d6'>[Bilibili]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:800px" src="./figures/general_full/general_full-1.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					Our event-based autofocus system consists of an event camera and a motorized varifocal lens. It leverages the proposed event-based focus measure and search method to focus the camera to the optimal focal position. When appropriately focused, the event camera's imaging result (b) is sharper and more informative than (a), (c) where it is defocused.
				</td>
			</tr>
		</table>
	</center>

	<hr>
	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Focus control (FC) is crucial for cameras to capture sharp images in challenging real-world scenarios. The autofocus (AF) facilitates the FC by automatically adjusting the focus settings. However, due to the lack of effective AF methods for the recently introduced event cameras, their FC still relies on naive AF like manual focus adjustments, leading to poor adaptation in challenging real-world conditions. In particular, the inherent differences between event and frame data in terms of sensing modality, noise, and temporal resolutions bring many challenges in designing an effective AF method for event cameras. To address these challenges, we develop a novel event-based autofocus framework consisting of an event-specific focus measure called event rate (ER) and a robust search strategy called event-based golden search (EGS). To verify the performance of our method, we have collected an event-based autofocus dataset (EAD) containing well-synchronized frames, events, and focal positions in a wide variety of challenging scenes with severe lighting and motion conditions. The experiments on this dataset and additional real-world scenarios demonstrated the superiority of our method over state-of-the-art approaches in terms of efficiency and accuracy. 
			</td>
		</tr>
	</table>
	<br>
	<hr>

	<center><h1>Pipeline</h1></center>
	<p align="center">
		<img class="round" style="width:1080px" src="./figures/pipeline_new/pipeline_new-1.png"/>
	</p>
	<table align=center width=1080px>
		<tr>
			<td>
				Our event-based autofocus system first traverses all possible focal positions from minimum to maximum to collect events data. (b) Then the system will use the event-based golden search (EGS) in cooperation with the event-based focus measure to find the optimal focal position and (c) adjust the lens accordingly. 
			</td>
		</tr>
	</table>

	<hr>
	<center><h1>CVPR 2022 Oral Talk</h1></center>
		<p align="center">
			<iframe width="660" height="395" src="https://www.youtube.com/embed/cgByn5ZYDw4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
		</p>
	<hr>
	<center><h1>Supp Video</h1></center>
		<p align="center">
			<iframe width="660" height="395" src="//player.bilibili.com/player.html?aid=810092080&bvid=BV1g34y147d6&cid=557460073&page=1" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
		</p>
	<hr>

	<center><h1>Dataset</h1></center>
	<p align="center">
		<img class="round" style="width:1080px" src="./figures/dataset_appendix_1/dataset_appendix_1-1.png"/>
		<img class="round" style="width:1080px" src="./figures/dataset_appendix_2/dataset_appendix_2-1.png"/>
		<img class="round" style="width:1080px" src="./figures/dataset_appendix_3/dataset_appendix_3-1.png"/>
		<img class="round" style="width:1080px" src="./figures/dataset_appendix_4/dataset_appendix_4-1.png"/>
		<img class="round" style="width:1080px" src="./figures/dataset_appendix_5/dataset_appendix_5-1.png"/>
		<img class="round" style="width:1080px" src="./figures/dataset_appendix_6/dataset_appendix_6-1.png"/>
	</p>



	<center><h1>Demo</h1></center>
	<p align="center">
		<img class="round" style="width:800px" src="./figures/extreme_1/extreme_1-1.png"/>
	</p>
	<p align="center">
		<img class="round" style="width:800px" src="./figures/extreme_2/extreme_2-1.png"/>
	</p>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					From left to right, our system accurately focus the camera in low light (marked by gray) in two situations with and without scenes motions.
				</td>
			</tr>
		</center>
	</table>

	<br>
	<hr>
	<table align=center width=450px>
		<center><h1>Citation</h1></center>
		<tr>
		Please cite our work if you find the dataset or our code helpful.
		<pre>
@inproceedings{lin2022autofocus,
	title={Autofocus for Event Cameras},
	author={Shijie, Lin and Yinqiang, Zhang and Lei, Yu and Bin, Zhou and Xiaowei, Luo and Jia, Pan},
	booktitle={The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	year={2022},
}</pre>
		</tr>
	</table>
	<br>
	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This project is supported by HKSAR RGC GRF 11202119, 11207818, T42-717/20-R, HKSAR Technology Commission under the InnoHK initiative,  National Natural Science Foundation of China, Grant 61871297, and the Natural Science Foundation of Hubei Province, China, Grant 2021CFB467.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

